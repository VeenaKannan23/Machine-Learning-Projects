# -*- coding: utf-8 -*-
"""
Created on Sat Sep 29 14:45:00 2018

@author: Admin
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
dataset = pd.read_csv('trainv1.csv')
d1 = pd.read_csv('testv1.csv')
x = dataset.iloc[:,2:96]
print(x)
y = dataset.iloc[:,96]
print(y)



#Model performance plot
def model_performance(cm):
    TP = cm[0][0]
    FN = cm[0][1]
    FP = cm[1][0]
    TN = cm[1][1]
    P = TP+FN
    N = FP+TN
    accuracy = (TP+TN)/float(P+N)*100
    error = (FP+FN)/float(P+N)*100
    sensitivity = TP/float(P)*100
    specificity = TN/float(N)*100
    print('TP: '+str(TP))
    print('TN: '+str(TN))
    print('FP: '+str(FP))
    print('FN: '+str(FN))
    print('P: '+str(P))
    print('N: '+str(N))
    print('accuracy: '+str(accuracy))
    print('error: '+str(error))
    print('sensitivity: '+str(sensitivity))
    print('specificity: '+str(specificity))

#Splitting the dataset into Test Set and Training Set
from sklearn.model_selection import train_test_split
x_train ,x_test ,y_train,y_test = train_test_split(x,y,test_size =0.25 ,random_state = 0)

#Ada Boost
from sklearn.ensemble import AdaBoostClassifier
AdaBoost_model = AdaBoostClassifier(n_estimators=700, random_state=0)
AdaBoost_model.fit(x_train, y_train)
y_pred = AdaBoost_model.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred)
print(cm)

model_performance(cm)

#Random Forest
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators = 30, random_state = 0, max_features = 0.5)

#rf_model.fit(pd.DataFrame(x_train),pd.DataFrame( y_train))
rf_model.fit(x_train, y_train)

test_pred = rf_model.predict(x_test)


from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,test_pred)
print(cm)
model_performance(cm)

#Bagging---------
from sklearn.ensemble import BaggingClassifier
bagging_model = BaggingClassifier(n_estimators=15, random_state=0)
bagging_model.fit(x_train, y_train)
test_pred = bagging_model.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,test_pred)
print(cm)
model_performance(cm)

#Decision Trees in Python
import numpy as np 
import pandas as pd 
from sklearn.metrics import confusion_matrix 
from sklearn.cross_validation import train_test_split 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report  

def train_using_gini(x_train, x_test, y_train): 
  
    # Creating the classifier object 
    clf_gini = DecisionTreeClassifier(criterion = "gini", 
            random_state = 100,max_depth=10, min_samples_leaf=5) 
    
    clf_gini.fit(x_train, y_train) 
    return clf_gini 

def prediction(X_test, clf_object): 
  
    # Predicton on test with giniIndex 
    y_pred = clf_object.predict(x_test) 
    print("Predicted values:") 
    print(y_pred) 
    return y_pred 

def prediction(X_test, clf_object): 
  
    # Predicton on test with giniIndex 
    y_pred = clf_object.predict(X_test) 
    print("Predicted values:") 
    print(y_pred) 
    return y_pred 
# Function to perform training with entropy. 
def tarin_using_entropy(X_train, X_test, y_train): 
  
    # Decision tree with entropy 
    clf_entropy = DecisionTreeClassifier( 
            criterion = "entropy", random_state = 100, 
            max_depth = 10, min_samples_leaf = 5) 
  
    # Performing training 
    clf_entropy.fit(X_train, y_train) 
    return clf_entropy 



clf_gini = train_using_gini(x_train, x_test, y_train) 
y_pred_gini = prediction(x_test, clf_gini)
cm = confusion_matrix(y_test,y_pred_gini)
print(cm)
model_performance(cm)

clf_entropy = tarin_using_entropy(x_train, x_test, y_train)
y_pred_entropy = prediction(x_test, clf_entropy)
cm = confusion_matrix(y_test,y_pred_entropy)
print(cm)
model_performance(cm)




#Naive B
  
# training a Naive Bayes classifier 
from sklearn.naive_bayes import GaussianNB 
gnb = GaussianNB().fit(x_train, y_train) 
gnb_predictions = gnb.predict(x_test) 
  
# accuracy on X_test 
accuracy = gnb.score(x_test, y_test) 
print (accuracy) 
  # creating a confusion matrix 
cm = confusion_matrix(y_test, gnb_predictions)
print(cm)  

#KNN for classification
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 4,metric = 'minkowski',p=2)
classifier.fit(x_train,y_train)
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
model_performance(cm)




# Gradient Boost
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report
gradientBoosting_model = GradientBoostingClassifier(n_estimators=10,
learning_rate=1.0,
max_depth=None,
random_state=0)
gradientBoosting_model.fit(x_train,y_train)
test_pred = gradientBoosting_model.predict(x_test)
cm = confusion_matrix(y_test, test_pred)
print(cm)
model_performance(cm)




#Visualising
#Mean education
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (10, 8))


sns.barplot(x = 'Target.corrected', y = 'instlevel', data = dataset);
plt.xlabel= "Poverty Levels"
plt.ylabel ="Level of Education"
plt.title('Education Distribution by Target');
plt.savefig('Educ.jpg')
plt.show()

#Number of households
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (10, 8))
sns.barplot(x = 'Target.corrected', y = 'r4h3', data = dataset);
plt.xlabel= "Poverty Levels"
plt.ylabel ="Level of Education"
plt.title('Total number of households by Target');
plt.savefig('rft3.jpg')
plt.show()

#Dependency
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (10, 8))
sns.pointplot(x = 'Target.corrected', y = 'dependency', data = dataset);
plt.xlabel= "Poverty Levels"
plt.ylabel ="Level of Education"
plt.title('Dependency vs Target');
plt.savefig('Dependency.jpg')
plt.show()

#mobilephone usage
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (10, 8))
sns.violinplot(x = 'Target.corrected', y = 'qmobilephone', data = dataset);
plt.xlabel= "Poverty Levels"
plt.ylabel ="Number of Mobile Phones Owned"
plt.title('Mobilephoneusage vs Target');
plt.savefig('Mobilephone.jpg')
plt.show()

#Overcrowding
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="ticks", color_codes=True)
plt.figure(figsize = (10, 8))
sns.pointplot(x = 'Target.corrected', y = 'v2a1', data = dataset);
plt.xlabel= "Poverty Levels"
plt.ylabel ="Level of overcrowding"
plt.title('Monthly_rent_payment vs Target');
plt.savefig('rent.jpg')
plt.show()

#



